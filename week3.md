# ML Track | Week 3
Navigate to [README](README.md) | [Week 2](week2.md) ⬅️ Week 3 ➡️ [Week 4](week4.md)
## Readings

- [Lecture 4 Slides](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture4.pdf)
- [Notes on backpropagation](https://cs231n.github.io/optimization-2/)
- [A case study for a two-layer neural network](https://cs231n.github.io/neural-networks-case-study/)
    - Specifically, [computing the analytic gradient for softmax](https://cs231n.github.io/neural-networks-case-study/#grad)


## Lecture Videos
* [Lecture 4 | Introduction to Neural Networks](https://www.youtube.com/watch?v=d14TUNcbn1k&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)
* (Optional) [3blue1brown's series of videos on Neural Networks](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)

## Homework
* [Implement a softmax classifier](assignments/colab/2020/module1/softmax.ipynb)
    * Check out the above reading on [computing the analytic gradient for softmax](https://cs231n.github.io/neural-networks-case-study/#grad)
    * Solutions
        * [softmax.ipynb](assignments/solutions/week3/softmax.ipynb)
        * [softmax.py](assignments/solutions/week3/softmax.py)
        * [Video explanation](https://youtu.be/47vHdLcDlT8)

## Workshop Recording
* [Week 3 Workshop](https://youtu.be/xPS31n0rZl4)
    * Solution to last week's SVM homework
